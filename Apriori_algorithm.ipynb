{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhkJMQ1WH0savgbgGUVMmr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paulin178/Machine-Learning/blob/main/Apriori_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fsf3dg0gCE6",
        "outputId": "5df49364-6ffb-4789-f61d-8ae6d47ce8cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itemsets Fréquents :\n",
            "     support     itemsets\n",
            "0   1.000000        (0.0)\n",
            "1   0.336164        (1.0)\n",
            "2   0.382664        (3.0)\n",
            "3   0.251241        (9.0)\n",
            "4   0.239443       (10.0)\n",
            "5   0.247794       (11.0)\n",
            "6   0.336164   (0.0, 1.0)\n",
            "7   0.382664   (0.0, 3.0)\n",
            "8   0.251241   (0.0, 9.0)\n",
            "9   0.239443  (0.0, 10.0)\n",
            "10  0.247794  (0.0, 11.0)\n",
            "\n",
            "Règles d'Association :\n",
            "  antecedents consequents  antecedent support  consequent support   support  \\\n",
            "0       (1.0)       (0.0)            0.336164                 1.0  0.336164   \n",
            "1       (3.0)       (0.0)            0.382664                 1.0  0.382664   \n",
            "2       (9.0)       (0.0)            0.251241                 1.0  0.251241   \n",
            "3      (10.0)       (0.0)            0.239443                 1.0  0.239443   \n",
            "4      (11.0)       (0.0)            0.247794                 1.0  0.247794   \n",
            "\n",
            "   confidence  lift  leverage  conviction  zhangs_metric  \n",
            "0         1.0   1.0       0.0         inf            0.0  \n",
            "1         1.0   1.0       0.0         inf            0.0  \n",
            "2         1.0   1.0       0.0         inf            0.0  \n",
            "3         1.0   1.0       0.0         inf            0.0  \n",
            "4         1.0   1.0       0.0         inf            0.0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "import warnings\n",
        "\n",
        "# Ignorer les avertissements de dépréciation\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Chargement des données (exemple de données)\n",
        "data = pd.read_csv('clear_data.csv')\n",
        "\n",
        "# Sélection des colonnes pertinentes\n",
        "selected_columns = ['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig',\n",
        "                    'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud', 'isFraud']\n",
        "data = data[selected_columns]\n",
        "\n",
        "# Suppression des lignes avec des valeurs manquantes\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Encodage des variables catégorielles avec LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_columns = ['type', 'nameOrig', 'nameDest']\n",
        "for col in categorical_columns:\n",
        "    data[col] = label_encoder.fit_transform(data[col])\n",
        "\n",
        "# Séparation des caractéristiques (X) et de la variable cible (y)\n",
        "X = data.drop('isFraud', axis=1)\n",
        "y = data['isFraud']\n",
        "\n",
        "# Division des données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Mise à l'échelle des données\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convertir les données en transactions pour Apriori\n",
        "transactions = X_train.apply(lambda row: row.dropna().tolist(), axis=1).tolist()\n",
        "\n",
        "# Application de l'algorithme Apriori\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
        "\n",
        "# Affichage des itemsets fréquents\n",
        "print(\"Itemsets Fréquents :\")\n",
        "print(frequent_itemsets)\n",
        "\n",
        "# Génération des règles d'association\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "# Affichage des règles d'association\n",
        "print(\"\\nRègles d'Association :\")\n",
        "print(rules)\n"
      ]
    }
  ]
}